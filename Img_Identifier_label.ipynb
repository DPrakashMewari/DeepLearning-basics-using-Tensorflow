{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to Make Person folder and Label wise Images\n",
    "# Create some folder to detect Your Friend Images \n",
    "\n",
    "# Image Normalize and trained via Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Step 3: Load and Preprocess the Dataset\n",
    "image_size = (64, 64)  # Resize images to a smaller size for simplicity\n",
    "batch_size = 2  # You can increase the batch size if you have more memory\n",
    "\n",
    "train_data_generator = ImageDataGenerator(rescale=1./255)  # Normalize pixel values\n",
    "train_data = train_data_generator.flow_from_directory(\n",
    "    directory='person_images',\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "num_classes = len(train_data.class_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 31, 31, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               1605760   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,625,668\n",
      "Trainable params: 1,625,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "7/7 [==============================] - 2s 120ms/step - loss: 1.3563 - accuracy: 0.5714\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 0.6485 - accuracy: 0.7857\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 1s 71ms/step - loss: 0.5999 - accuracy: 0.7143\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.3713 - accuracy: 0.8571\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 1s 68ms/step - loss: 0.3280 - accuracy: 0.9286\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1396 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0545 - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 1s 77ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 9.3972e-04 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 1s 80ms/step - loss: 6.3087e-04 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 4.8488e-04 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 3.8944e-04 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 3.4311e-04 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 3.0568e-04 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 2.8496e-04 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 2.5925e-04 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 2.4109e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2145bb9b710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Define the CNN Model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(image_size[0], image_size[1], 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Step 5: Compile the Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the Model\n",
    "epochs = 20  # Increase the number of epochs if needed\n",
    "model.fit(train_data, epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the weights of the first convolutional layer (assuming it is the first layer)\n",
    "conv1_layer = model.get_layer(index=0)\n",
    "conv1_weights = conv1_layer.get_weights()[0]\n",
    "\n",
    "# Normalize the weights to [0, 1] for visualization\n",
    "normalized_weights = (conv1_weights - np.min(conv1_weights)) / (np.max(conv1_weights) - np.min(conv1_weights))\n",
    "\n",
    "# Plot the filters as images\n",
    "num_filters = conv1_weights.shape[-1]\n",
    "fig, axs = plt.subplots(4, 8, figsize=(12, 6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i in range(num_filters):\n",
    "    axs[i].imshow(normalized_weights[:, :, 0, i], cmap='gray')\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted Person: aysh\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as  plt \n",
    "test_image_path = 'test2.png'\n",
    "test_image = tf.keras.preprocessing.image.load_img(test_image_path, target_size=image_size)\n",
    "# img = cv2.imread(test_image_path)\n",
    "# cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "test_image = tf.keras.preprocessing.image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0) / 255.0  # Normalize the test image\n",
    "\n",
    "# Make the prediction\n",
    "prediction = model.predict(test_image)\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# Get the class labels from the training data generator\n",
    "class_labels = list(train_data.class_indices.keys())\n",
    "\n",
    "# Get the predicted person's name\n",
    "predicted_person = class_labels[predicted_class]\n",
    "\n",
    "print(\"Predicted Person:\", predicted_person)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
